import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('Indicadores_municipales_sabana_DA.csv', index_col=0, sep=',', encoding='latin-1')
print(df)

missing_data_per_column = df.isnull().sum()
#Total
total_missing = df.isnull().sum().sum()
print('\nTotal of null data in the dataset:', total_missing)

#Columns
columns_with_missing_data = missing_data_per_column[missing_data_per_column > 0].count()
print('Number of columns with null data:', columns_with_missing_data)

#Categorical to binary
df = pd.get_dummies(df, columns=['gdo_rezsoc00','gdo_rezsoc05', 'gdo_rezsoc10'])
print(df)

# Calculate mean per group for numeric columns
grouped_subsets = {group: group_df for group, group_df in df.groupby('nom_ent')}

numeric_columns = df.select_dtypes(include=np.number).columns
df[numeric_columns] = df.groupby('nom_ent')[numeric_columns].transform(lambda x: x.fillna(x.mean()))

#Count total null data again to verify
missing_data_per_column = df.isnull().sum()
total_missing = df.isnull().sum().sum()
print('\nTotal null data in the data set:', total_missing)


# Desired feature
feature_deseado = 'N_carencias3'

# Calculate the correlation matrix between the desired feature and the other features.
correlation_matrix = df.corr()[[feature_deseado]]

#Correlation matrix using a heatmap
plt.figure(figsize=(50, 50))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)
plt.title('Matriz de Correlación para ' + feature_deseado)
plt.show()

#Outliers

numeric_columns = df.select_dtypes(include=np.number).columns

# Calculate the interquartile range for each numerical column
Q1 = df[numeric_columns].quantile(0.25)
Q3 = df[numeric_columns].quantile(0.75)
IQR = Q3 - Q1

# Identifying outliers
outliers = (df[numeric_columns] < (Q1 - 1.5 * IQR)) | (df[numeric_columns] > (Q3 + 1.5 * IQR))

# Deleting outliers
df = df[~outliers.any(axis=1)]


#Regression Model

# Selecting the target variable and features
target_variable = 'N_carencias3'
features = ['pobtot_ajustada', 'N_pobreza', 'N_pobreza_e','N_pobreza_m','N_vul_car','N_vul_ing','N_npnv','N_ic_rezedu','N_ic_asalud','N_ic_segsoc','N_ic_cv','N_ic_sbv','N_ic_ali','N_carencias','N_plb','N_plb_m','pobtot_00','pobtot_05','pobtot_10']

# Create the feature dataset (X) and the target variable (y)
X = df[features].values
y = df[target_variable].values

# Split data (80% train, 20% test)
split_ratio = 0.8
split_index = int(len(X) * split_ratio)

X_train, X_test = X[:split_index], X[split_index:]
y_train, y_test = y[:split_index], y[split_index:]

# Train the linear regression model
X_train_transposed = X_train.T
X_train_transposed_with_bias = np.vstack([X_train_transposed, np.ones(len(X_train))]).T

# Calculate the coefficients
coefficients = np.linalg.inv(X_train_transposed_with_bias.T.dot(X_train_transposed_with_bias)).dot(X_train_transposed_with_bias.T).dot(y_train)

# Make predictions on the test set
X_test_with_bias = np.vstack([X_test.T, np.ones(len(X_test))]).T
y_pred = X_test_with_bias.dot(coefficients)

# Calculate mean square error
mse = np.mean((y_test - y_pred) ** 2)

# Calculate the Coefficient of Determination
y_mean = np.mean(y_test)
sst = np.sum((y_test - y_mean) ** 2)
ssr = np.sum((y_test - y_pred) ** 2)
r2 = 1 - (ssr / sst)

print('Mean Squared Error (MSE):', mse)
print('Coefficient of Determination (R^2):', r2)

#Predictions
for i in range(len(y_test)):
    print(f'Real Value: {y_test[i]}, Prediction: {y_pred[i]}')

# Graphic Lineal Regression
plt.scatter(y_test, y_pred, color='blue', label='Data')


plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', label='Regression Line')
plt.xlabel('Real Values')
plt.ylabel('Predictions')
plt.title('Linear Regression Graph: Real Data vs. Predictions')
plt.legend()
plt.show()

#Linear regression model using klearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_error, r2_score

# Selecting the target variable and features
target_variable = 'N_carencias3'
features = ['pobtot_ajustada', 'N_pobreza', 'N_pobreza_e','N_pobreza_m','N_vul_car','N_vul_ing','N_npnv','N_ic_rezedu','N_ic_asalud','N_ic_segsoc','N_ic_cv','N_ic_sbv','N_ic_ali','N_carencias','N_plb','N_plb_m','pobtot_00','pobtot_05','pobtot_10']


# Create the feature dataset (X) and the target variable (y)
X = df[features].values
y = df[target_variable].values

# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('Mean Squared Error (MSE):', mse)
print('Coeficiente de determinación (R^2):', r2)

# Predictions
for i in range(len(y_test)):
    print(f'Valor Real: {y_test[i]}, Predicción: {y_pred[i]}')

# Graphic
plt.scatter(y_test, y_pred, color='blue', label='Data')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', label='Regression Line')
plt.xlabel('Real values')
plt.ylabel('Predictions')
plt.title('Sklearn Linear Regression Graph: Actual Data vs. Predictions')
plt.legend()
plt.show()
