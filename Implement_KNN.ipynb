{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMawXQEj7loI9b3p+bVjIkC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariquintal/machinelearning/blob/main/Implement_KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **KNN**"
      ],
      "metadata": {
        "id": "nGPPxxe_qGgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors (KNN) is a supervised learning algorithm that is based on the idea that data points that are close in the feature space have similar labels. To predict the label of an unknown data point, KNN finds the K nearest data points or \"neighbors\" in the training dataset and assigns the most common label among those neighbors to the unknown data point."
      ],
      "metadata": {
        "id": "ljpvqK6FWq3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pseudocode**"
      ],
      "metadata": {
        "id": "Y6JS5-9dm4PQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Load dataset\n",
        "Split data into features (X) and labels (y)\n",
        "\n",
        "Define test set size (20%)\n",
        "\n",
        "Set random seed for reproducibility\n",
        "\n",
        "Split data into training and test sets\n",
        "\n",
        "Define KNN classifier\n",
        "\n",
        "- Initialize number of nearest neighbors (n_neighbors)\n",
        "\n",
        "- Fit(X_train, y_train)\n",
        "\n",
        "  - Store X_train and y_train\n",
        "\n",
        "- Predict(X_query)\n",
        "  - Calculate distances between X_query and X_train\n",
        "\n",
        " - Find indices of nearest neighbors\n",
        "\n",
        "  - Get labels of nearest neighbors\n",
        "\n",
        " - Determine most common label for each point, y_pred\n",
        "\n",
        "Create and train KNN classifier\n",
        "\n",
        "Predict labels for test set\n",
        "\n",
        "Calculate accuracy\n",
        "\n",
        "Print classification results for each class\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tOr-xvl4VJ1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar el conjunto de datos\n",
        "df = pd.read_csv(\"diabetes.csv\", sep=\",\")\n",
        "\n",
        "# Separar features (X) y etiquetas (y)\n",
        "X = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI','DiabetesPedigreeFunction','Age']].values\n",
        "y = df['Outcome'].values\n",
        "\n",
        "# Definir el tamaño del conjunto de prueba (20%)\n",
        "test_size = 0.2\n",
        "num_samples = len(df)\n",
        "num_test_samples = int(test_size * num_samples)\n",
        "\n",
        "# Fijar la semilla para reproducibilidad\n",
        "np.random.seed(42)\n",
        "\n",
        "# Obtener índices aleatorios para el conjunto de prueba\n",
        "test_indices = np.random.choice(num_samples, num_test_samples, replace=False)\n",
        "\n",
        "# Crear conjuntos de entrenamiento y prueba\n",
        "X_train, y_train = np.delete(X, test_indices, axis=0), np.delete(y, test_indices)\n",
        "X_test, y_test = X[test_indices], y[test_indices]\n",
        "\n",
        "# Definir el clasificador KNN\n",
        "class KNNClassifier:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train, self.y_train = X_train, y_train\n",
        "\n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2, axis=1))\n",
        "\n",
        "    def predict(self, X_query):\n",
        "        distances = np.sqrt(np.sum((X_query[:, np.newaxis] - self.X_train) ** 2, axis=2))\n",
        "        k_nearest_neighbors = np.argsort(distances)[:, :self.n_neighbors]\n",
        "        k_nearest_labels = self.y_train[k_nearest_neighbors]\n",
        "        # Obtener la clase que más aparece (moda)\n",
        "        y_pred = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=k_nearest_labels)\n",
        "        return y_pred\n",
        "\n",
        "# Crear y entrenar el clasificador KNN\n",
        "knn = KNNClassifier(n_neighbors=31)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predecir las etiquetas para el conjunto de prueba y obtener la clase que más aparece\n",
        "y_pred_mode = knn.predict(X_test)\n",
        "predicted_mode = np.bincount(y_pred_mode).argmax()\n",
        "\n",
        "# Calcular y mostrar la precisión (accuracy)\n",
        "accuracy = np.mean(y_pred_mode == y_test)\n",
        "\n",
        "# Imprimir la clase que más aparece y la precisión\n",
        "print(\"Predicted Class:\")\n",
        "print(predicted_mode)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lup1T3sVGHhq",
        "outputId": "be957753-95a6-467e-d75b-9de0f1e2926c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class:\n",
            "0\n",
            "Accuracy: 0.7647058823529411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loss Function**"
      ],
      "metadata": {
        "id": "GEg31V-Iz1V6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A loss function is like a score that tells us how well our model is doing. It measures the difference between what our model predicts and what's actually true. We use this score to adjust our model and make it better at its task."
      ],
      "metadata": {
        "id": "Yye2EUz_zZUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Optimization function identification**"
      ],
      "metadata": {
        "id": "qNG9p3Kp0CEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimization function identification means picking the best way to improve and adjust a model during training so that it becomes really good at its job."
      ],
      "metadata": {
        "id": "HV4bfF-f0FZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html KNN.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSyGZSec3ajv",
        "outputId": "b6ae0a2b-25ba-42b0-90f3-faa732bb0b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook KNN.ipynb to html\n",
            "[NbConvertApp] Writing 591150 bytes to KNN.html\n"
          ]
        }
      ]
    }
  ]
}